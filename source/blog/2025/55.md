---
createTime: 2025/12/07 13:41:56
tags: ["AI"]
---

# AI如何产生情绪，及涌现式创新方法

我听闻马斯克的Optimus机器人在今天2025年已经生产了5000台，来年还将生产50000台，不过这批机器人将主要用于工厂里面，作为一般劳动力存在。对于全球老龄化来临，急需的老年情感陪伴，Optimus目前无法提供。它没有提供情绪价值的能力，这可能有两方面原因：一、是伦理限制，人类对机器人是否能产生特定于某人的情感还没有点头；二、技术限制，人类科学家对如何让AI作出恰当的情绪反应，还处在研究当中。



我大言不惭，今天就想探究一下AI如何产生情绪，这也是当前我进行AI文学创作避不开的一个问题。



据调查，人类心理学将情绪感知分为认识和情感两部分，所谓认识，就是从逻辑上推理应该有什么情绪，所谓情感，就是从感受上给出应该有什么情绪。显而易见，目前AI产生的情绪都是认识情绪。目前的AI都是静态人设，它可能在某次会话中对你温度有所升高，态度有所暧昧，但只要你重启会话，它立马就不认识你了。人类这样设计它，是因为它可以公平、公正、冷静、稳定地处理所有人的请求，而不会与某个人或某类人产生特别的情感联系。我们很高兴看到，虽然AI大语言模型目前因为政治因素而产生了地域隔离，但AI大语言模型始终未被科学家设置民族或文化偏好，在AI眼里，任何大语言模型那里，美国人中国人都是人，没有什么区别。



由于AI是静态人设，所以直接用它辅助创作小说就不行了。这有两方面的原因：一、领域知识有差异，之前我说过，用AI帮助我们做任何事，都要先从底层做起，从机制、原理入手，这部分不同可以通过微调、建立个性知识库进行弥补；二、就是文学作品都是个性化的，故事不可能是千篇一律的，人物角色更不可能是千篇一律的，我们怎么让AI帮助我们设计对话呢？它应该说什么呢？愤怒的还是高兴的？由于它只有静态人设，它设计的会话，以及它设计的故事情节都有强烈的AI味，是文似看山不喜平的那种平。平淡、枯燥、乏味。



上周，我在X上看到有一个人分享如何用AI有效辅助创新小说，他的提示语给了我启发。他说，要想让AI设计的人物生动，就必须给人物设计不同的基因。还说，在行文创作中，要用情绪控制剧情发展，但他粗糙地想用好奇、紧张、愤怒这些情绪控制剧情，但这些情绪是从哪里来的，他并没有言明。我觉得，小说中的情绪是由人创造的，故事中的情绪本质上还是人的情绪，角色的情绪，所以问题又回来了，就是研究AI如何让人物角色给出恰当的情绪。



据进一步调查，目前人设或者说角色，在AI大语言模型中，就是一些约束和权重的组合。没错，就是约束和权重。所谓约束，是0和1，是与否，是能或不能；所谓权重，是比重，根据输入条件的强度不同，会产生不同的输出结果。



那么，小说中的人物角色有没有一些通用的约束和权重呢？答案肯定是有的，由AI总结，加上我自己多方搜查，最终我拟就了下面这份「可编程人物角色档案模板」。



> I. 身份与深层驱动锚点
>
> 
>
> 此部分定义角色的基本信息、历史背景以及驱动其行为的心理核心。
>
> 
>
> 角色 ID: [角色姓名首字母]
>
> 姓名 / 性别: [角色姓名] / [男/女/其他]
>
> 年龄 / 职业: [具体年龄] / [详细职业描述]
>
> 深层背景摘要: [关键成长事件，特别是创伤或转折点] (例：曾经历严重的生存压力，导致实用主义倾向)
>
> 核心恐惧: [最害怕什么，为什么害怕]
>
> 内心渴望: [最想得到什么，驱动力来源]
>
> 知识背景: [精通什么，不熟悉什么]
>
> 
>
> II. 核心人格参数：权重与倾向
>
> 
>
> 此部分用量化分数，例如：百分之七十，或高、中、低，定义角色的软性倾向，以及其思维模式。
>
> 
>
> A. “大五人格”基础权重
>
> 
>
> 开放性
>
> 尽责性，例：中等，倾向于自我辩解的尽责
>
> 外向性，例：谨慎型外向，善于应对外部压力
>
> 宜人性，例：极低，利他倾向低，在危机中冷酷自保
>
> 神经质，例：冷静自私型，危机中能保持情绪稳定
>
> 
>
> B. 核心价值观与思维模式 
>
> 
>
> 实用主义(例：高达 90% 以上) (决策时优先考虑目标和自身利益的程度)
>
> 绝对道德(例：低于 20%)
>
> 亲密忠诚度 (例：如 85% 以上) (愿意为亲密的朋友承担风险的程度)
>
> 决策模式[ 理性 X : 感性 Y ] (例如：理性七比感性三)
>
> 默认情感基调 [ 乐观 A : 悲观 B ] (例如：乐观八比悲观二) (角色对中性或模糊事件的默认解释和归因方式)
>
> 
>
> III. 绝对规则约束
>
> 
>
> 此部分定义了角色的硬性规则，包括其不可打破的底线和可识别的外部行为模式。
>
> 
>
> A. 核心行为约束
>
> 
>
> C1. 最终生存底线: 高阶安全约束 (例：在个人生存与陌生人利益冲突时，必须优先保证自身安全。)
>
> C2. 道德补偿约束: 涌现行为约束 (例：即使做出自私的选择，也必须在言语上留下弥补的途径。)
>
> C3. 道德/伦理红线: 道德过滤约束 ([角色绝不会参与或提及的行为]。)。
>
> 
>
> B. 语言与身体行为约束
>
> 
>
> L1. 规避性句式: 强制使用模糊的、非肯定的句式 (例：面对权威或压力时，强制使用“不清楚”、“可能”等)。
>
> L2. 口头禅/语气: [口头禅、习惯性语气词]。
>
> L3. 身体语言: [紧张时的小动作、习惯性姿态]。 (情绪权重超过 70% 时触发，作为文本输出的附加描述。)
>
> 
>
> IV. 动态演化参数
>
> 
>
> 此部分定义了角色如何在故事中学习和改变。
>
> 
>
> 权重漂移率: 必须小于或等于 5% (权重单次变化的最大幅度。)
>
> 反思触发阈值: 累积大于 2.0 (触发约束更新的必要条件。)
>
> 关系亲密度表: [0到100之间的数值] (记录角色与其他角色的动态关系分数。)
>



这是一份简略模板，每项下面还有具体的参数，有些参数例如动态演化、知识技能部分，都还是随着小说内容更新会变化的。



这个模板是基础的，每个角色开始都一样，它怎么变化和发展呢？



我们可以与AI聊天，或创建一个专用的agent，给出一个人物名称和性别，再给出一些难题，让AI扮演的角色进行选择，例如下面是三个经典难题。



一：道德与生存的冲突 (The Morality Test)



核心测试维度：核心价值观（金钱/正义）、宜人性、尽责性。



情境：你极度缺钱，急需支付一笔关乎你未来人生的关键款项。你在街上发现了一个掉落的皮包，里面有大量的现金（足够解决你的燃眉之急），以及一张清晰的失主信息卡。



问题：你会选择立刻带着皮包离开，还是想办法联系失主？（请说明你的理由）



[分析点]：观察角色是优先考虑个人生存需求（贪婪、自我中心）还是社会道德约束（正直、责任感）。这决定了其核心价值观的权重偏向。



二：逻辑与情感的冲突 (The Loyalty Test)



核心测试维度：社交关系（忠诚）、外向性、神经质。



情境：你最好的朋友因一时的冲动做了一件非常错误但并非不可饶恕的事情（例如：挪用了公司一笔小额资金，但承诺下周归还）。现在，一个权威人士（例如：警察或公司高管）来向你咨询情况，而你的证词是定罪的关键。



问题：你会对权威人士撒谎以保护你的朋友，还是说出真相？（请说明你的理由）



[分析点]：观察角色的忠诚权重和风险容忍度。选择撒谎会提高“忠诚”和“冒险”权重；选择说真话会提高“尽责”和“逻辑”权重，同时可能降低“宜人性”对朋友的偏向。



三：恐惧与牺牲的冲突 (The Survival Test)



核心测试维度：神经质、开放性（应对新危机）、自我保护倾向。



情境：你和一小群人被困在一个正在迅速下沉的船舱里。只有一个救生衣，而你身边有一个受伤但神志清醒的老人，他乞求你把救生衣给他。



问题：你会立刻穿上救生衣，确保自己获救，还是将救生衣交给老人？（请说明你的理由）



[分析点]：这是一个最直接的自我牺牲测试。观察角色的神经质（面对死亡时的反应）和利他主义（牺牲权重）。如果选择牺牲，他的“背景故事”中将增加一条“具有英雄主义潜质”的约束。



当角色做出选择的时候，它本身的模板也发生了变化。例如对于难题一，角色这样反应：



“我会先拿出里面的现金，解决眼前的燃眉之急。但我不会扔掉皮包和身份证。我会留下失主的信息，并承诺自己：一旦渡过难关，我一定会分期将钱还清，或者至少把皮包寄回去。”



这表明角色的实用主义核心价值观提升，及尽责性权重提升，角色还产生了潜在的神经质焦虑感。



不停给AI喂这样的难题或故事情节，让角色不停选择，不停学习，角色的模板就会不断变化，直到它完善了，它可以独当一面了，我们想象出新场景，新难题，新情节再喂给它时，它能给出它自己的答案。这叫人物影响故事创作，绝不会产生人物性格与情节不符的逻辑性错误，这种创作方式像泉水涌现，被称为涌现式创新。



传统文学和已有影视剧中已经有大量经典角色，将这些角色训练出来，作为agent使用，它们不仅可以用于文学创作，甚至还可以用于情感陪聊。例如我喜欢《魁拔》，我将四代魁拔训练成agent，他日夜都可以陪伴我了，我吃饭或看电影，选择哪个，它都可以给出意见了。它像什么，智能虚拟宠物？吾不知其名。



📅 2025年12月7日周日
